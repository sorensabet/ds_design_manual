{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-13: How to algorithmically judge if a Turker is doing their job in sentiment classification task? \n",
    "\n",
    "My first approach would be to label a small subset of reviews myself (e.g. 500-1000), then distribute these in the data I am sending to the labellers. I can then compare the labeller performance against my own, and if their labels are significantly different than my own, this is a signal that they are not reviewing them. \n",
    "\n",
    "Another approach would be to send the same records to multiple labellers, then compare their labels. This approach assumes that most of the labellers are doing their job correctly, but if one reviewer's labels are consistently different than the rest on a subset of the data, this is a signal that something is off.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
